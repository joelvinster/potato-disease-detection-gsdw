{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "cweoNnndkWjq",
        "outputId": "5f03bfb0-ea9c-4fba-a400-9f0bf7c6aa6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1j0taLQAqvSYjArb6jm40FhzFbG1yxhRs\n",
            "From (redirected): https://drive.google.com/uc?id=1j0taLQAqvSYjArb6jm40FhzFbG1yxhRs&confirm=t&uuid=3bc50f53-e48f-4896-aece-a600aa0bee19\n",
            "To: /content/archive.zip\n",
            "100% 753M/753M [00:10<00:00, 71.2MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown --fuzzy https://drive.google.com/file/d/1j0taLQAqvSYjArb6jm40FhzFbG1yxhRs/view?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umopGgItkmqi"
      },
      "outputs": [],
      "source": [
        "!apt install p7zip-full -y\n",
        "!7z x /content/archive.zip -o/content/dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XcWnBjsQkqBP"
      },
      "outputs": [],
      "source": [
        "!pip install torchsampler\n",
        "!pip install torchmetrics\n",
        "!pip install timm\n",
        "!pip install split-folders\n",
        "!pip install thop\n",
        "!pip install einops\n",
        "!pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cpu/torch1.11.0/index.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIFQEsHUktMK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "#Load libraries\n",
        "import numpy as np\n",
        "import glob\n",
        "from torch.optim import Adam\n",
        "from torch.autograd import Variable\n",
        "import torchvision\n",
        "import pathlib\n",
        "import torchvision.models as models\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import accuracy_score,f1_score,precision_score,recall_score,confusion_matrix, classification_report,roc_auc_score,roc_curve, auc\n",
        "from torchsampler import ImbalancedDatasetSampler\n",
        "from torch.utils.data.sampler import BatchSampler\n",
        "import random\n",
        "import timm\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import Linear, ReLU, LeakyReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
        "#import cupy as cp\n",
        "import seaborn as sns\n",
        "import torchmetrics\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from tqdm.notebook import tqdm, trange\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from mmcv.cnn import constant_init, kaiming_init\n",
        "from thop import profile\n",
        "from einops import rearrange\n",
        "from einops.layers.torch import Rearrange, Reduce\n",
        "from timm.models.layers import trunc_normal_, DropPath\n",
        "import splitfolders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JoHK1f2ylG_Q"
      },
      "outputs": [],
      "source": [
        "seed = 123\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjEbmddemFQP"
      },
      "outputs": [],
      "source": [
        "# Define input dataset path (adjust accordingly)\n",
        "input_folder = \"/content/Potato_Leaf_Disease_Dataset\"\n",
        "\n",
        "# Define output directory where split data will be saved\n",
        "output_folder = \"/content/data\"\n",
        "\n",
        "# Split dataset into train (70%), validation (20%), and test (10%)\n",
        "splitfolders.ratio(input_folder, output=output_folder, seed=1337, ratio=(0.7, 0.2, 0.1))\n",
        "\n",
        "# Define new folder paths\n",
        "train_folder = \"/content/data/train\"\n",
        "val_folder = \"/content/data/val\"\n",
        "test_folder = \"/content/data/test\"\n",
        "\n",
        "print(\"Dataset split completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEhFgm07mIBG"
      },
      "outputs": [],
      "source": [
        "#Transforms\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(90),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4ulISd4mo_F"
      },
      "outputs": [],
      "source": [
        "# Datasets\n",
        "train_dataset = torchvision.datasets.ImageFolder(root=train_path, transform=train_transforms)\n",
        "val_dataset = torchvision.datasets.ImageFolder(root=val_path, transform=val_transforms)\n",
        "test_dataset = torchvision.datasets.ImageFolder(root=test_path, transform=test_transforms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0-YTtlvmq70"
      },
      "outputs": [],
      "source": [
        "train_dataset.class_to_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "waSMk3gHmsoe"
      },
      "outputs": [],
      "source": [
        "class BalancedBatchSampler(torch.utils.data.sampler.Sampler):\n",
        "    def __init__(self, dataset, labels=None):\n",
        "        self.labels = labels\n",
        "        self.dataset = dict()\n",
        "        self.balanced_max = 0\n",
        "        # Save all the indices for all the classes\n",
        "        for idx in range(0, len(dataset)):\n",
        "            label = self._get_label(dataset, idx)\n",
        "            if label not in self.dataset:\n",
        "                self.dataset[label] = list()\n",
        "            self.dataset[label].append(idx)\n",
        "            self.balanced_max = len(self.dataset[label]) \\\n",
        "                if len(self.dataset[label]) > self.balanced_max else self.balanced_max\n",
        "\n",
        "        # Oversample the classes with fewer elements than the max\n",
        "        for label in self.dataset:\n",
        "            while len(self.dataset[label]) < self.balanced_max:\n",
        "                self.dataset[label].append(random.choice(self.dataset[label]))\n",
        "        self.keys = list(self.dataset.keys())\n",
        "        self.currentkey = 0\n",
        "        self.indices = [-1]*len(self.keys)\n",
        "\n",
        "    def __iter__(self):\n",
        "        while self.indices[self.currentkey] < self.balanced_max - 1:\n",
        "            self.indices[self.currentkey] += 1\n",
        "            yield self.dataset[self.keys[self.currentkey]][self.indices[self.currentkey]]\n",
        "            self.currentkey = (self.currentkey + 1) % len(self.keys)\n",
        "        self.indices = [-1]*len(self.keys)\n",
        "\n",
        "    def _get_label(self, dataset, idx, labels = None):\n",
        "        if self.labels is not None:\n",
        "            return self.labels[idx].item()\n",
        "        else:\n",
        "            # Trying guessing\n",
        "            dataset_type = type(dataset)\n",
        "            if is_torchvision_installed and dataset_type is torchvision.datasets.MNIST:\n",
        "                return dataset.train_labels[idx].item()\n",
        "            elif is_torchvision_installed and dataset_type is torchvision.datasets.ImageFolder:\n",
        "                return dataset.imgs[idx][1]\n",
        "            else:\n",
        "                raise Exception(\"You should pass the tensor of labels to the constructor as second argument\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.balanced_max*len(self.keys)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PB-XYbXmw8b"
      },
      "outputs": [],
      "source": [
        "train_loader = torch.utils.data.DataLoader(dataset = train_dataset, sampler=ImbalancedDatasetSampler(train_dataset), batch_size=4)\n",
        "val_loader = torch.utils.data.DataLoader(dataset = val_dataset, batch_size=4, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset = test_dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "#train_eg_loader = torch.utils.data.DataLoader(dataset = train_eg, batch_size=1, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgCcE_pFmzgN"
      },
      "outputs": [],
      "source": [
        "is_torchvision_installed = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FCJbWvSmz5p"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrAHG2G3m145"
      },
      "outputs": [],
      "source": [
        "def weights_init(m):\n",
        "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
        "        nn.init.xavier_uniform_(m.weight.data)\n",
        "        nn.init.zeros_(m.bias.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyJMyPq1m3so"
      },
      "outputs": [],
      "source": [
        "def last_zero_init(m):\n",
        "    if isinstance(m, nn.Sequential):\n",
        "        constant_init(m[-1], val=0)\n",
        "    else:\n",
        "        constant_init(m, val=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r39eFj4dm5On"
      },
      "outputs": [],
      "source": [
        "class GDSW (nn.Module):\n",
        "    def __init__ (self, dim_in, dim_out):\n",
        "        super(GDSW, self).__init__()\n",
        "\n",
        "        self.gc1 = nn.Conv2d (dim_in, 1668, kernel_size = (3,3),padding = 1, groups = 3)\n",
        "        self.cs = channel_shuffle (groups = 3)\n",
        "        self.DSWC = depthwise_separable_conv (1668, 1671)\n",
        "        self.gc2 = nn.Conv2d (1671, dim_out, kernel_size = (3, 3),stride = 2, padding = 1, groups = 3)\n",
        "\n",
        "    def forward (self, x):\n",
        "        #print(\"input to gsdw\",x.shape)\n",
        "        x = self.gc1 (x)\n",
        "       # print(\"gconv1\",x.shape)\n",
        "        x = self.cs(x)\n",
        "        #print(\" channel shuffle \",x.shape)\n",
        "        x = self.DSWC (x)\n",
        "        #print(\"depthwise\",x.shape)\n",
        "        x = self.gc2(x)\n",
        "        #print(\"gconv2\",x.shape)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ab0qkkHlm7NE"
      },
      "outputs": [],
      "source": [
        "\n",
        "class GDSW2 (nn.Module):\n",
        "    def __init__ (self, dim_in, dim_out):\n",
        "        super(GDSW2, self).__init__()\n",
        "\n",
        "        self.gc1 = nn.Conv2d (dim_in, 1686, kernel_size = (3,3), padding = 1, groups = 3)\n",
        "        self.cs = channel_shuffle (groups = 3)\n",
        "        self.DSWC = depthwise_separable_conv (1686, 1689)\n",
        "        self.gc2 = nn.Conv2d (1689, dim_out, kernel_size = (3,3), stride = 2,padding = 1, groups = 3)\n",
        "\n",
        "    def forward (self, x):\n",
        "       # print(\"input to gsdw\",x.shape)\n",
        "        x = self.gc1 (x)\n",
        "       # print(\"gconv1\",x.shape)\n",
        "        x = self.cs(x)\n",
        "       # print(\" channel shuffle \",x.shape)\n",
        "        x = self.DSWC (x)\n",
        "       # print(\"depthwise\",x.shape)\n",
        "        x = self.gc2(x)\n",
        "       # print(\"gconv2\",x.shape)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7O8DhK0m9ID"
      },
      "outputs": [],
      "source": [
        "class GDSW3 (nn.Module):\n",
        "    def __init__ (self, dim_in, dim_out):\n",
        "        super(GDSW3, self).__init__()\n",
        "\n",
        "        self.gc1 = nn.Conv2d (dim_in, 1692, kernel_size = (3,3), padding = 1, groups = 3)\n",
        "        self.cs = channel_shuffle (groups = 3)\n",
        "        self.DSWC = depthwise_separable_conv (1692,1767)\n",
        "        self.gc2 = nn.Conv2d (1767, dim_out, kernel_size = (3,3),stride = 2, padding = 1, groups = 3)\n",
        "\n",
        "    def forward (self, x):\n",
        "        #print(\"input to gsdw\",x.shape)\n",
        "        x = self.gc1 (x)\n",
        "        #print(\"gconv1\",x.shape)\n",
        "        x = self.cs(x)\n",
        "        #print(\" channel shuffle \",x.shape)\n",
        "        x = self.DSWC (x)\n",
        "       # print(\"depthwise\",x.shape)\n",
        "        x = self.gc2(x)\n",
        "       # print(\"gconv2\",x.shape)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zz-yo7Zgm-nm"
      },
      "outputs": [],
      "source": [
        "class depthwise_separable_conv(nn.Module):\n",
        "    def __init__(self, nin, nout):\n",
        "        super(depthwise_separable_conv, self).__init__()\n",
        "        self.depthwise = nn.Conv2d(nin, nin, kernel_size=3, padding=1, groups=nin)\n",
        "        #self.depthwise = nn.Conv2d(nin, nin, kernel_size=1, padding=1, groups=nin)\n",
        "        self.pointwise = nn.Conv2d(nin, nout, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.depthwise(x)\n",
        "        out = self.pointwise(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ghGDkITsnAEi"
      },
      "outputs": [],
      "source": [
        "class channel_shuffle (nn.Module):\n",
        "    def __init__(self, groups):\n",
        "        super (channel_shuffle, self).__init__()\n",
        "\n",
        "        self.groups = groups\n",
        "\n",
        "    def forward (self, x):\n",
        "\n",
        "        batchsize, num_channels, height, width = x.size()\n",
        "        channels_per_group = num_channels // self.groups\n",
        "        x = x.view(batchsize, self.groups, channels_per_group, height, width)\n",
        "        x = torch.transpose(x, 1, 2).contiguous()\n",
        "        x = x.view(batchsize, -1, height, width)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NqAMbTolnBc6"
      },
      "outputs": [],
      "source": [
        "class BasicConv(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1, groups=1, relu=True, bn=True, bias=False):\n",
        "        super(BasicConv, self).__init__()\n",
        "        self.out_channels = out_planes\n",
        "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n",
        "        self.bn = nn.BatchNorm2d(out_planes,eps=1e-5, momentum=0.01, affine=True) if bn else None\n",
        "        self.relu = nn.ReLU() if relu else None\n",
        "\n",
        "    def forward(self, x):\n",
        "        #self.conv = self.conv.half()\n",
        "        x = self.conv(x)\n",
        "        if self.bn is not None:\n",
        "            x = self.bn(x)\n",
        "        if self.relu is not None:\n",
        "            x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "class ZPool(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return torch.cat( (torch.max(x,1)[0].unsqueeze(1), torch.mean(x,1).unsqueeze(1)), dim=1)\n",
        "\n",
        "class AttentionGate(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AttentionGate, self).__init__()\n",
        "        kernel_size = 7\n",
        "        self.compress = ZPool()\n",
        "        self.conv = BasicConv(2, 1, kernel_size, stride=1, padding=(kernel_size-1) // 2, relu=False)\n",
        "    def forward(self, x):\n",
        "        x_compress = self.compress(x)\n",
        "        x_out = self.conv(x_compress)\n",
        "        scale = torch.sigmoid_(x_out)\n",
        "        return x * scale\n",
        "\n",
        "class TripletAttention(nn.Module):\n",
        "    def __init__(self, no_spatial=False):\n",
        "        super(TripletAttention, self).__init__()\n",
        "        self.cw = AttentionGate()\n",
        "        self.hc = AttentionGate()\n",
        "        self.no_spatial=no_spatial\n",
        "        if not no_spatial:\n",
        "            self.hw = AttentionGate()\n",
        "    def forward(self, x):\n",
        "        x_perm1 = x.permute(0,2,1,3).contiguous()\n",
        "        x_out1 = self.cw(x_perm1)\n",
        "        x_out11 = x_out1.permute(0,2,1,3).contiguous()\n",
        "        x_perm2 = x.permute(0,3,2,1).contiguous()\n",
        "        x_out2 = self.hc(x_perm2)\n",
        "        x_out21 = x_out2.permute(0,3,2,1).contiguous()\n",
        "        if not self.no_spatial:\n",
        "            x_out = self.hw(x)\n",
        "            x_out = 1/3 * (x_out + x_out11 + x_out21)\n",
        "        else:\n",
        "            x_out = 1/2 * (x_out11 + x_out21)\n",
        "        return x_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0q_ZC6RVnEOY"
      },
      "outputs": [],
      "source": [
        "# ---- Model ----\n",
        "class PotatoDiseaseModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PotatoDiseaseModel, self).__init__()\n",
        "        self.backbone = models.densenet169(pretrained=True)\n",
        "        num_ftrs = self.backbone.classifier.in_features\n",
        "\n",
        "        # Remove classifier but keep feature maps\n",
        "        self.features = self.backbone.features\n",
        "        self.sh = GDSW(1668,1680)\n",
        "        self.batchn = nn.BatchNorm2d(1680)\n",
        "        self.sh2 = GDSW2(1680,1689) #1686\n",
        "        self.batchn2 = nn.BatchNorm2d(1689)\n",
        "        self.sh3 = GDSW3(1689,1767) #1692\n",
        "        self.batchn3 = nn.BatchNorm2d(1767)\n",
        "        #self.gcn = GlobalContextBlock(1664,2)\n",
        "        self.triplet = TripletAttention()\n",
        "        self.channel_upscale = nn.Conv2d(1664, 1668, kernel_size=1, stride=1, padding=0)\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1,1))\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d((1,1)),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(1767, 7))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)       # [batch, 1664, H, W]\n",
        "        x1 = self.channel_upscale(x)  # Correctly increases channels from 1664 → 1668\n",
        "        x1 = F.interpolate(x1, size=(64, 64), mode='bilinear', align_corners=True)\n",
        "        x1 = self.sh(x1)\n",
        "        #print(\"x1 first GSDW:\",x1.shape)\n",
        "        x1 = self.batchn(x1)\n",
        "\n",
        "        #GSDW block2\n",
        "        x1 = self.sh2(x1)\n",
        "        #print(\"x1 second GSDW:\",x1.shape)\n",
        "        x1 = self.batchn2(x1)\n",
        "\n",
        "        #GSDW block3\n",
        "        x1 = self.sh3(x1)\n",
        "        #print(\"x1 third GSDW:\",x1.shape)\n",
        "        x1 = self.batchn3(x1)\n",
        "       # x = self.gcn(x)            # still [batch, 1664, H, W]\n",
        "        x1 = self.triplet(x1)        # still [batch, 1664, H, W]\n",
        "        x1 = self.classifier(x1)\n",
        "        return x1\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JoYqN041nHZl"
      },
      "outputs": [],
      "source": [
        "model=PotatoDiseaseModel().to(device)\n",
        "#Optmizer and loss function\n",
        "#optimizer=Adam(model.parameters(),lr=0.001,weight_decay=0.0001)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9, weight_decay=0.001)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "loss_function=torch.nn.CrossEntropyLoss()  #change to focal loss\n",
        "scaler = GradScaler(enabled=True)\n",
        "use_cuda = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AfzJp4ylnI1_"
      },
      "outputs": [],
      "source": [
        "train_accu = []\n",
        "training_loss = []\n",
        "\n",
        "def train(model, iterator, optimizer, criterion, device):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    image_preds_all = []\n",
        "    image_targets_all = []\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for (x, y) in tqdm(iterator, desc=\"Training\", leave=False):\n",
        "\n",
        "        x = x.to(device).float()\n",
        "        y = y.to(device).long()\n",
        "\n",
        "        with autocast():\n",
        "\n",
        "            y_pred = model(x)\n",
        "\n",
        "            image_preds_all += [torch.argmax(y_pred, 1).detach().cpu().numpy()]\n",
        "            image_targets_all += [y.detach().cpu().numpy()]\n",
        "\n",
        "            loss = criterion(y_pred, y)\n",
        "\n",
        "            acc = calculate_accuracy(y_pred, y)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            #torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
        "\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "    image_preds_all = np.concatenate(image_preds_all)\n",
        "    image_targets_all = np.concatenate(image_targets_all)\n",
        "    score = (image_preds_all==image_targets_all).mean()\n",
        "\n",
        "    train_losss = epoch_loss / len(iterator)\n",
        "\n",
        "    train_accu.append(score*100)\n",
        "    training_loss.append(train_losss)\n",
        "\n",
        "    #print(score)\n",
        "    #print(len(train_accu), len(training_loss))\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator), train_accu, training_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRQC_aXNnKjc"
      },
      "outputs": [],
      "source": [
        "val_accu = []\n",
        "eval_loss = []\n",
        "\n",
        "def evaluate(model, iterator, criterion, device):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    image_preds_all = []\n",
        "    image_targets_all = []\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for (x, y) in tqdm(iterator, desc=\"Evaluating\", leave=False):\n",
        "\n",
        "            x = x.to(device).float()\n",
        "            y = y.to(device).long()\n",
        "\n",
        "            y_pred = model(x)\n",
        "\n",
        "            image_preds_all += [torch.argmax(y_pred, 1).detach().cpu().numpy()]\n",
        "            image_targets_all += [y.detach().cpu().numpy()]\n",
        "\n",
        "            loss = criterion(y_pred, y)\n",
        "\n",
        "            acc = calculate_accuracy(y_pred, y)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "\n",
        "    image_preds_all = np.concatenate(image_preds_all)\n",
        "    image_targets_all = np.concatenate(image_targets_all)\n",
        "    score = (image_preds_all==image_targets_all).mean()\n",
        "\n",
        "    val_losss = epoch_loss / len(iterator)\n",
        "\n",
        "    val_accu.append(score*100)\n",
        "    eval_loss.append(val_losss)\n",
        "\n",
        "    performance_matrix(image_targets_all, image_preds_all)\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator), val_accu, eval_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSxHSc1anL9v"
      },
      "outputs": [],
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kRzpj9gnNQZ"
      },
      "outputs": [],
      "source": [
        "def calculate_accuracy(y_pred, y):\n",
        "    top_pred = y_pred.argmax(1, keepdim=True)\n",
        "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
        "    acc = correct.float() / y.shape[0]\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFDnZCoYnOkz"
      },
      "outputs": [],
      "source": [
        "def performance_matrix(true,pred):\n",
        "    precision = precision_score(true,pred,average='macro')\n",
        "    recall = recall_score(true,pred,average='macro')\n",
        "    accuracy = accuracy_score(true,pred)\n",
        "    f1_sco = f1_score(true,pred,average='macro')\n",
        "    #print('Confusion Matrix:\\n',confusion_matrix(true, pred))\n",
        "    print('Precision: {:.4f} Recall: {:.4f}, Accuracy: {:.4f}: ,f1_score: {:.4f}'.format(precision,recall,accuracy,f1_sco))\n",
        "    print('Classification Report:\\n',classification_report(true, pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pha41dRYnRbC"
      },
      "outputs": [],
      "source": [
        "def checkpoint_model(epoch, model, opt, model_path):\n",
        "    model_state_dict = model.state_dict() if (device.type == 'cuda') else model.state_dict()\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model_state_dict,\n",
        "        'opt_state_dict': opt.state_dict(),\n",
        "        'best_val_acc': best_val_acc\n",
        "    }, model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XyzYwv6jnS22"
      },
      "outputs": [],
      "source": [
        "\n",
        "EPOCHS = 50\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "best_val_acc = 0.\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "\n",
        "train_acc_gr = []\n",
        "train_loss_gr = []\n",
        "val_acc_gr = []\n",
        "val_loss_gr = []\n",
        "dresults = pd.DataFrame(columns=['Epoch','Train_Acc', 'Train_loss','Val_Acc', 'Val_loss'])\n",
        "\n",
        "#ckp_path = r\"C:\\Users\\dell 5370\\Desktop\\try\\CNN_base_epoch40.pth\"\n",
        "#model, optimizer, start_epoch = load_ckp(ckp_path, model, optimizer)\n",
        "\n",
        "#model.load_state_dict(torch.load(ckp_path), strict=False)\n",
        "for epoch in trange(EPOCHS, desc=\"Epochs\"):\n",
        "\n",
        "    start_time = time.monotonic()\n",
        "\n",
        "    train_loss, train_acc, train_acc_gr, train_loss_gr = train(model, train_loader, optimizer, criterion, device)\n",
        "    valid_loss, valid_acc, val_acc_gr, val_loss_gr = evaluate(model, val_loader, criterion, device)\n",
        "    dresults = pd.concat([dresults, pd.DataFrame([{\n",
        "    'Epoch': epoch+1,\n",
        "    'Train_Acc': train_acc,\n",
        "    'Train_loss': train_loss,\n",
        "    'Val_Acc': valid_acc,\n",
        "    'Val_loss': valid_loss\n",
        "    }])], ignore_index=True)\n",
        "\n",
        "    #dresults = dresults.append({'Epoch': epoch+1,'Train_Acc': train_acc, 'Train_loss':train_loss,'Val_Acc': valid_acc, 'Val_loss':valid_loss},ignore_index=True)\n",
        "    # if valid_loss < best_valid_loss:\n",
        "    #     best_valid_loss = valid_loss\n",
        "    #     torch.save(model.state_dict(), '/home/ubuntu/Code/Alzheimers/Results/Model weights/base_4layer.pt')\n",
        "    if epoch % 10 == 0:\n",
        "        checkpoint_model(epoch, model, optimizer, '/kaggle/working/CNN_base_epoch%d.pth' % epoch)\n",
        "\n",
        "    end_time = time.monotonic()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
        "dresults.to_csv('/content/dresults.csv', index=False)\n",
        "\n",
        "plt.plot(train_acc_gr,'-o')\n",
        "plt.plot(val_acc_gr,'-o')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend(['Train','Valid'])\n",
        "plt.title('Train vs Valid Accuracy')\n",
        "#plt.savefig('/home/administrator/Desktop//accuracy.png')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(train_loss_gr,'-o')\n",
        "plt.plot(val_loss_gr,'-o')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('losses')\n",
        "plt.legend(['Train','Valid'])\n",
        "plt.title('Train vs Valid Losses')\n",
        "#plt.savefig('/home/administrator/Desktop/loss.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_Z6T4EZnTbL"
      },
      "outputs": [],
      "source": [
        "epoch_loss = 0\n",
        "epoch_acc = 0\n",
        "image_preds_all = []\n",
        "image_targets_all = []\n",
        "\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    for (x1, y1) in tqdm(test_loader, desc=\"Evaluating\", leave=False):\n",
        "\n",
        "        x1 = x1.to(device).float()\n",
        "        y = y1.to(device).long()\n",
        "\n",
        "        y_pred =model(x1)\n",
        "\n",
        "        image_preds_all += [torch.argmax(y_pred, 1).detach().cpu().numpy()]\n",
        "        image_targets_all += [y.detach().cpu().numpy()]\n",
        "\n",
        "        loss = criterion(y_pred, y)\n",
        "\n",
        "        acc = calculate_accuracy(y_pred, y)\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "\n",
        "image_preds_all = np.concatenate(image_preds_all)\n",
        "image_targets_all = np.concatenate(image_targets_all)\n",
        "score = (image_preds_all==image_targets_all).mean()\n",
        "\n",
        "performance_matrix(image_targets_all, image_preds_all)\n",
        "\n",
        "#plot_roc(image_targets_all, image_preds_all, N_classes=3)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}